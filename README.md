# çŸ¥è­˜è’¸é¤¾ï¼ˆKnowledge Distillationï¼‰æ•™å­¸å°ˆæ¡ˆ

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ä¸€å€‹å®Œæ•´çš„çŸ¥è­˜è’¸é¤¾ï¼ˆKnowledge Distillationï¼‰æ•™å­¸å°ˆæ¡ˆï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨å¤§å‹æ•™å¸«æ¨¡å‹çš„è¼¸å‡ºä¾†è¨“ç·´å°å‹å­¸ç”Ÿæ¨¡å‹ï¼Œå¯¦ç¾æ¨¡å‹å£“ç¸®åŒæ™‚ä¿æŒé«˜æ•ˆèƒ½ã€‚

## ğŸ“š ä»€éº¼æ˜¯çŸ¥è­˜è’¸é¤¾ï¼Ÿ

çŸ¥è­˜è’¸é¤¾æ˜¯ä¸€ç¨®æ¨¡å‹å£“ç¸®æŠ€è¡“ï¼Œç”± Hinton ç­‰äººåœ¨ 2015 å¹´æå‡ºã€‚æ ¸å¿ƒæ¦‚å¿µæ˜¯ï¼š

- **æ•™å¸«æ¨¡å‹ï¼ˆTeacher Modelï¼‰**ï¼šå¤§å‹ã€é«˜æ•ˆèƒ½çš„æ¨¡å‹
- **å­¸ç”Ÿæ¨¡å‹ï¼ˆStudent Modelï¼‰**ï¼šå°å‹ã€è¼•é‡çš„æ¨¡å‹
- **è»Ÿæ¨™ç±¤ï¼ˆSoft Labelsï¼‰**ï¼šæ•™å¸«æ¨¡å‹è¼¸å‡ºçš„æ©Ÿç‡åˆ†ä½ˆï¼ŒåŒ…å«æ›´è±å¯Œçš„ã€Œæš—çŸ¥è­˜ã€
- **æº«åº¦åƒæ•¸ï¼ˆTemperatureï¼‰**ï¼šæ§åˆ¶æ©Ÿç‡åˆ†ä½ˆçš„å¹³æ»‘ç¨‹åº¦ï¼Œæº«åº¦è¶Šé«˜åˆ†ä½ˆè¶Šå¹³æ»‘

é€éè®“å­¸ç”Ÿæ¨¡å‹å­¸ç¿’æ•™å¸«æ¨¡å‹çš„è»Ÿæ¨™ç±¤ï¼Œå¯ä»¥ç²å¾—æ¯”ç›´æ¥è¨“ç·´æ›´å¥½çš„æ•ˆèƒ½ã€‚

## ğŸ¯ å°ˆæ¡ˆç‰¹è‰²

- âœ… å®Œæ•´çš„çŸ¥è­˜è’¸é¤¾å¯¦ä½œæµç¨‹
- âœ… æ¨¡çµ„åŒ–çš„ç¨‹å¼ç¢¼æ¶æ§‹
- âœ… è©³ç´°çš„ä¸­æ–‡è¨»è§£
- âœ… æ”¯æ´ Google Colab é‹è¡Œ
- âœ… è¦–è¦ºåŒ–è¨“ç·´éç¨‹
- âœ… CIFAR-10 ç¤ºç¯„ç¯„ä¾‹
- âœ… æ˜“æ–¼æ“´å±•åˆ°å…¶ä»–è³‡æ–™é›†

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
distill/
â”œâ”€â”€ data/                      # è³‡æ–™ç›¸é—œæ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ datasets.py           # è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
â”œâ”€â”€ models/                    # æ¨¡å‹å®šç¾©
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ teacher.py            # æ•™å¸«æ¨¡å‹ï¼ˆResNet18ï¼‰
â”‚   â””â”€â”€ student.py            # å­¸ç”Ÿæ¨¡å‹ï¼ˆè¼•é‡ CNNï¼‰
â”œâ”€â”€ distillation/             # çŸ¥è­˜è’¸é¤¾æ ¸å¿ƒ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loss.py               # è’¸é¤¾æå¤±å‡½æ•¸
â”‚   â””â”€â”€ trainer.py            # è¨“ç·´å™¨
â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py            # è¼”åŠ©å‡½æ•¸
â”œâ”€â”€ notebooks/                 # Jupyter ç­†è¨˜æœ¬
â”‚   â””â”€â”€ distillation_tutorial.ipynb  # å®Œæ•´æ•™å­¸
â”œâ”€â”€ requirements.txt           # å¥—ä»¶éœ€æ±‚
â”œâ”€â”€ setup.py                   # å®‰è£é…ç½®
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ–¹æ³• 1ï¼šåœ¨ Google Colab ä¸Šé‹è¡Œï¼ˆæ¨è–¦ï¼‰

1. é–‹å•Ÿ Colabï¼š[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com)

2. ä¸Šå‚³æˆ–é€£çµåˆ° `notebooks/distillation_tutorial.ipynb`

3. æŒ‰ç…§ notebook ä¸­çš„æ­¥é©ŸåŸ·è¡Œå³å¯

### æ–¹æ³• 2ï¼šæœ¬åœ°ç’°å¢ƒé‹è¡Œ

#### å®‰è£ä¾è³´

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/yourusername/knowledge-distillation.git
cd knowledge-distillation

# å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆå»ºè­°ï¼‰
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate  # Windows

# å®‰è£ä¾è³´å¥—ä»¶
pip install -r requirements.txt

# æˆ–ä½¿ç”¨ setup.py å®‰è£
pip install -e .
```

## ğŸ’¡ ä½¿ç”¨ç¯„ä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from data import get_cifar10_dataloaders
from models import TeacherModel, StudentModel
from distillation import DistillationLoss, DistillationTrainer
from utils import set_seed

# è¨­å®šéš¨æ©Ÿç¨®å­
set_seed(42)

# è¼‰å…¥è³‡æ–™
train_loader, test_loader = get_cifar10_dataloaders(batch_size=128)

# å»ºç«‹æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
teacher_model = TeacherModel(num_classes=10).to(device)
student_model = StudentModel(num_classes=10).to(device)

# é…ç½®çŸ¥è­˜è’¸é¤¾
distillation_loss = DistillationLoss(temperature=3.0, alpha=0.7)
optimizer = torch.optim.SGD(student_model.parameters(), lr=0.1, momentum=0.9)

# å»ºç«‹è¨“ç·´å™¨
trainer = DistillationTrainer(
    teacher_model=teacher_model,
    student_model=student_model,
    train_loader=train_loader,
    test_loader=test_loader,
    distillation_loss=distillation_loss,
    optimizer=optimizer,
    device=device
)

# é–‹å§‹è¨“ç·´
history = trainer.train(num_epochs=50)
```

### è¦–è¦ºåŒ–çµæœ

```python
from utils import plot_training_curves

# ç¹ªè£½è¨“ç·´æ›²ç·š
plot_training_curves(history, save_path='./results/training_curves.png')
```

### æ¨¡å‹æ¯”è¼ƒ

```python
from utils import compare_models

# æ¯”è¼ƒæ•™å¸«å’Œå­¸ç”Ÿæ¨¡å‹çš„å¤§å°
compare_models(teacher_model, student_model)
```

## ğŸ”§ æ ¸å¿ƒåƒæ•¸èªªæ˜

### è’¸é¤¾æå¤±åƒæ•¸

- **temperature**ï¼ˆæº«åº¦åƒæ•¸ï¼‰
  - ç¯„åœï¼š1.0 - 10.0
  - é è¨­ï¼š3.0
  - èªªæ˜ï¼šæ§åˆ¶è»Ÿæ¨™ç±¤çš„å¹³æ»‘ç¨‹åº¦ï¼Œè¶Šé«˜è¶Šå¹³æ»‘

- **alpha**ï¼ˆè’¸é¤¾æ¬Šé‡ï¼‰
  - ç¯„åœï¼š0.0 - 1.0
  - é è¨­ï¼š0.7
  - èªªæ˜ï¼šè’¸é¤¾æå¤±èˆ‡å­¸ç”Ÿæå¤±çš„å¹³è¡¡ï¼Œalpha=1 è¡¨ç¤ºåªç”¨è’¸é¤¾æå¤±

### è¨“ç·´åƒæ•¸

- **learning_rate**ï¼šåˆå§‹å­¸ç¿’ç‡ï¼Œå»ºè­° 0.1
- **momentum**ï¼šå‹•é‡ï¼Œå»ºè­° 0.9
- **weight_decay**ï¼šæ¬Šé‡è¡°æ¸›ï¼Œå»ºè­° 5e-4
- **batch_size**ï¼šæ‰¹æ¬¡å¤§å°ï¼Œå»ºè­° 128
- **num_epochs**ï¼šè¨“ç·´è¼ªæ•¸ï¼Œå»ºè­° 50-100

## ğŸ“Š æ•ˆèƒ½åŸºæº–

åœ¨ CIFAR-10 è³‡æ–™é›†ä¸Šçš„æ¸¬è©¦çµæœï¼š

| æ¨¡å‹ | åƒæ•¸é‡ | æº–ç¢ºç‡ | å£“ç¸®æ¯” |
|------|--------|--------|--------|
| æ•™å¸«æ¨¡å‹ï¼ˆResNet18ï¼‰ | ~11M | 92-94% | 1.0x |
| å­¸ç”Ÿæ¨¡å‹ï¼ˆç„¡è’¸é¤¾ï¼‰ | ~0.5M | 85-87% | 22x |
| å­¸ç”Ÿæ¨¡å‹ï¼ˆæœ‰è’¸é¤¾ï¼‰ | ~0.5M | 89-91% | 22x |

**çµè«–**ï¼šçŸ¥è­˜è’¸é¤¾å¯ä»¥å°‡å­¸ç”Ÿæ¨¡å‹çš„æ•ˆèƒ½æå‡ç´„ 4-5%ï¼ŒåŒæ™‚ä¿æŒ 22 å€çš„å£“ç¸®æ¯”ã€‚

## ğŸ§ª å¯¦é©—èˆ‡èª¿åƒå»ºè­°

### æº«åº¦åƒæ•¸ï¼ˆTemperatureï¼‰

```python
# å¯¦é©—ä¸åŒçš„æº«åº¦
for temp in [1.0, 3.0, 5.0, 10.0]:
    loss_fn = DistillationLoss(temperature=temp, alpha=0.7)
    # è¨“ç·´ä¸¦æ¯”è¼ƒçµæœ
```

**å»ºè­°**ï¼š
- å°è³‡æ–™é›†ï¼šT = 3.0 - 5.0
- å¤§è³‡æ–™é›†ï¼šT = 5.0 - 10.0
- è¤‡é›œä»»å‹™ï¼šä½¿ç”¨è¼ƒé«˜æº«åº¦

### Alpha åƒæ•¸

```python
# å¯¦é©—ä¸åŒçš„ alpha å€¼
for alpha in [0.3, 0.5, 0.7, 0.9]:
    loss_fn = DistillationLoss(temperature=3.0, alpha=alpha)
    # è¨“ç·´ä¸¦æ¯”è¼ƒçµæœ
```

**å»ºè­°**ï¼š
- æ•™å¸«æ¨¡å‹å¾ˆå¼·ï¼šä½¿ç”¨è¼ƒé«˜ alpha (0.7-0.9)
- æ¨™ç±¤å“è³ªé«˜ï¼šä½¿ç”¨è¼ƒä½ alpha (0.3-0.5)
- é€šå¸¸æƒ…æ³ï¼šalpha = 0.7 æ˜¯å€‹å¥½çš„èµ·é»

## ğŸ“– é€²éšæ‡‰ç”¨

### 1. ä½¿ç”¨è‡ªå·±çš„è³‡æ–™é›†

```python
from torch.utils.data import DataLoader
from torchvision import transforms, datasets

# å®šç¾©è³‡æ–™è½‰æ›
transform = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])

# è¼‰å…¥è‡ªå·±çš„è³‡æ–™é›†
train_dataset = datasets.ImageFolder('path/to/train', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
```

### 2. ä½¿ç”¨è‡ªè¨‚æ¨¡å‹

```python
class MyTeacher(nn.Module):
    # å®šç¾©ä½ çš„æ•™å¸«æ¨¡å‹
    pass

class MyStudent(nn.Module):
    # å®šç¾©ä½ çš„å­¸ç”Ÿæ¨¡å‹
    pass
```

### 3. å¤šæ•™å¸«è’¸é¤¾

```python
# ä½¿ç”¨å¤šå€‹æ•™å¸«æ¨¡å‹
teacher_models = [teacher1, teacher2, teacher3]

# åœ¨è¨“ç·´è¿´åœˆä¸­å¹³å‡æ•™å¸«è¼¸å‡º
teacher_logits = torch.stack([
    teacher(inputs) for teacher in teacher_models
]).mean(dim=0)
```

## ğŸ¤ è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“ æˆæ¬Š

æœ¬å°ˆæ¡ˆä½¿ç”¨ MIT æˆæ¬Šæ¢æ¬¾ - è©³è¦‹ [LICENSE](LICENSE) æª”æ¡ˆ

## ğŸ™ è‡´è¬

- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) - Hinton et al., 2015
- PyTorch åœ˜éšŠæä¾›çš„å„ªç§€æ·±åº¦å­¸ç¿’æ¡†æ¶
- CIFAR-10 è³‡æ–™é›†

## ğŸ“§ è¯çµ¡æ–¹å¼

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œæ­¡è¿ï¼š
- é–‹å•Ÿ Issue
- ç™¼é€ Pull Request
- è¯çµ¡ç¶­è­·è€…ï¼šyour.email@example.com

## ğŸŒŸ ç›¸é—œè³‡æº

- [PyTorch å®˜æ–¹æ–‡ä»¶](https://pytorch.org/docs/)
- [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)
- [Model Compression ç›¸é—œè«–æ–‡](https://github.com/awesome-model-compression)

---

**äº«å—çŸ¥è­˜è’¸é¤¾çš„å­¸ç¿’ä¹‹æ—…ï¼** ğŸš€
